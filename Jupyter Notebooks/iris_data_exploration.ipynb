{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Pkg\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"StatsPlots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"GRDIR\"]=\"\"; Pkg.build(\"GR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.add(\"Pkg\")\n",
    "Pkg.add(\"GR\")\n",
    "Pkg.build(\"GR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using Plots\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "using StatsPlots\n",
    "using Dates\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_2018 = CSV.read(\"../EMS_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(ems_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = names(ems_2018)\n",
    "for i in 1:31\n",
    "    println(string(i), \"\\t\", string(feature_names[i]), \"\\t\\t\\t\", string(eltype(ems_2018[!, i])))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting to look at initial vs final severity\n",
    "# initial call type vs final call type (any outliers?)\n",
    "# Activation/dispatch Response times\n",
    "# histogram of incidents by date/time, borough/zipcode, policeprecinct, districts\n",
    "# could try joining income with district/schooldistrict\n",
    "# what is held_indicator, incident_disposition_code, reopen indicator, special event indicator, standby or transfer indicator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into months to run more quickly\n",
    "ems_jan2020 = CSV.read(\"EMS_2020_1.csv\")\n",
    "head(ems_jan2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = names(ems_jan2020)\n",
    "for i in 1:31\n",
    "    println(string(i), \"\\t\", string(feature_names[i]), \"\\t\\t\\t\", string(eltype(ems_jan2020[!, i])))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_jan2020 = ems_jan2020[ems_jan2020[:VALID_DISPATCH_RSPNS_TIME_INDC] .== \"Y\", :]\n",
    "ems_jan2020 = ems_jan2020[ems_jan2020[:VALID_INCIDENT_RSPNS_TIME_INDC] .== \"Y\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ismissing(ems_jan2020[:FIRST_HOSP_ARRIVAL_DATETIME])) #Doesn't recognize missing vals yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count missing for each row\n",
    "#histogram \n",
    "\"\"\"\n",
    "INCIDENT_RESPONSE_SECONDS_QY\t\t\tString\n",
    "14\tINCIDENT_TRAVEL_TM_SECONDS_QY\t\t\tString\n",
    "16\tFIRST_HOSP_ARRIVAL_DATETIME\t\t\tString\n",
    "19\tINCIDENT_DISPOSITION_CODE\t\t\tFloat64\n",
    "Try it as output, if drags accuracy down too much then leave out \n",
    "Maybe can use for training? But dont require as input for predictions \"\"\"\n",
    "\n",
    "\"\"\"transporting patient\n",
    "patient pronounced dead\n",
    "cancelled\n",
    "unfounded\n",
    "condition corrected\n",
    "treated not transported\n",
    "refused medical aid\n",
    "treated and transported\n",
    "triaged at scene no transport\n",
    "patient gone on arrival\n",
    "cancelled\n",
    "duplicate incident - Probably want to filter this?\n",
    "unit not sent\n",
    "no disposition\n",
    "\"\"\"\n",
    "@df ems_jan2020 corrplot([:INCIDENT_RESPONSE_SECONDS_QY, :INCIDENT_TRAVEL_TM_SECONDS_QY, :FIRST_HOSP_ARRIVAL_DATETIME, :FINAL_CALL_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?corrplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap(randn(10,10)) use if inputs are same values in similar range\n",
    "#x, y , z. x is the categories for the x axis, y is categories on y axis, z is the value/heat at each pair x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function string_to_float(str)\n",
    "    try\n",
    "        parse(Float64, str)\n",
    "    catch\n",
    "       0.0\n",
    "    end\n",
    "end\n",
    "ems_jan2020[:INCIDENT_RESPONSE_SECONDS_QY] = string_to_float.(ems_jan2020[:, :INCIDENT_RESPONSE_SECONDS_QY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_jan2020[:INCIDENT_TRAVEL_TM_SECONDS_QY] = string_to_float.(ems_jan2020[:, :INCIDENT_TRAVEL_TM_SECONDS_QY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(ems_jan2020[:INCIDENT_RESPONSE_SECONDS_QY], label=\"Distribution of EMS response times\")\n",
    "xlabel!(\"Response time (seconds)\")\n",
    "ylabel!(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The mean squared error of the true value y and prediction pred.\"\"\"\n",
    "function MSE(y, pred)\n",
    "    mse = sqrt(sum((y - pred).^2)/length(y))\n",
    "    return mse\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby zipcode to compute avg/variance in response time.\n",
    "avg_rspns_time_zip = by(ems_jan2020, :ZIPCODE, :INCIDENT_RESPONSE_SECONDS_QY => mean)\n",
    "var_rspns_time_zip = by(ems_jan2020, :ZIPCODE, :INCIDENT_RESPONSE_SECONDS_QY => var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df avg_rspns_time_zip scatter(:ZIPCODE, :INCIDENT_RESPONSE_SECONDS_QY_mean)\n",
    "xlabel!(\"zipcode\")\n",
    "ylabel!(\"average response time (seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df var_rspns_time_zip scatter(:ZIPCODE, :INCIDENT_RESPONSE_SECONDS_QY_var)\n",
    "xlabel!(\"zipcode\")\n",
    "ylabel!(\"variance in response time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df avg_rspns_time_zip scatter(:ZIPCODE, :INCIDENT_RESPONSE_SECONDS_QY_mean) #Want to calculate mean for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing - replace INCIDENT_DATETIME with month, year, time of day\n",
    "ems_jan2020[:INCIDENT_DATETIME] = map(x -> DateTime(x[:INCIDENT_DATETIME], \"m/d/y I:M:S p\"), eachrow(ems_jan2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymh = [Dates.year.(ems_jan2020[:INCIDENT_DATETIME]) Dates.month.(ems_jan2020[:INCIDENT_DATETIME]) Dates.hour.(ems_jan2020[:INCIDENT_DATETIME])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan2020timeresp = [ems_jan2020[:INCIDENT_RESPONSE_SECONDS_QY] ems_jan2020[:ZIPCODE] ymh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby time, then groupby zipcode. Then take the mean/variance of that\n",
    "by(ems_jan2020, :ZIPCODE, :INCIDENT_RESPONSE_SECONDS_QY => var)\n",
    "by(ems_jan2020, :ZIPCODE, by(ems_jan2020, :HOUR, :INCIDENT_RESPONSE_SECONDS_QY => mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(ems_jan2020[:INCIDENT_TRAVEL_TM_SECONDS_QY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby zipcode to compute avg/variance in response time.\n",
    "avg_trvl_time_zip = by(ems_jan2020, :ZIPCODE, :INCIDENT_TRAVEL_TM_SECONDS_QY => mean)\n",
    "var_trvl_time_zip = by(ems_jan2020, :ZIPCODE, :INCIDENT_TRAVEL_TM_SECONDS_QY => var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df avg_trvl_time_zip scatter(:ZIPCODE, :INCIDENT_TRAVEL_TM_SECONDS_QY_mean)\n",
    "xlabel!(\"zipcode\")\n",
    "ylabel!(\"average travel time (seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df var_trvl_time_zip scatter(:ZIPCODE, :INCIDENT_TRAVEL_TM_SECONDS_QY_var)\n",
    "xlabel!(\"zipcode\")\n",
    "ylabel!(\"variance in travel time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_2019 = CSV.read(\"EMS_2019_subsampled.csv\")\n",
    "head(ems_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST_HOSP_ARRIVAL_DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use incident disposition code for multiclass classification?\n",
    "#predict number of incidences in a time period (maybe also pred which hospitals) to help hospitals manage resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest algorithm, KNN because data doesnt look linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in excel data->filter->column\n",
    "#summarystats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(unique(ems_jan2020[:ZIPCODE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function string_to_float(str)\n",
    "    try\n",
    "        parse(Float64, str)\n",
    "    catch\n",
    "       0.0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Computes a onehot vector for every entry in column given a set of categories cats\"\n",
    "function onehot(column, cats=unique(column))\n",
    "    result = zeros(size(column, 1), size(cats,1))\n",
    "    for col_idx = 1:size(column,1)\n",
    "        for cats_idx = 1:size(cats,1)\n",
    "            comparison = column[col_idx] == cats[cats_idx]\n",
    "            \n",
    "            if (!ismissing(comparison) && comparison)\n",
    "                result[col_idx, cats_idx] = 1\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Dates\n",
    "\n",
    "function parse_date(ds::AbstractString)\n",
    "    fmt = Dates.DateFormat(\"mm/dd/yyyy HH:MM:SS\")\n",
    "    \n",
    "    m = match(r\"(.*?)\\s*(AM|PM)?$\"i, ds)\n",
    "    d = Dates.DateTime(m.captures[1], fmt)\n",
    "    ampm = uppercase(something(m.captures[2], \"\"))\n",
    "    d + Dates.Hour(12 * +(ampm == \"PM\", ampm == \"\" || Dates.hour(d) != 12, -1))\n",
    "end\n",
    "\n",
    "function process_dates(datetimes)\n",
    "    dates = parse_date.(datetimes)\n",
    "    \n",
    "    years = Dates.year.(dates)\n",
    "    months = Dates.month.(dates)\n",
    "    weeks = Dates.week.(dates)\n",
    "    days = Dates.day.(dates)\n",
    "    hours = Dates.hour.(dates);\n",
    "   return hcat(years, months, weeks, days, hours) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# convert dates\n",
    "# onehot encoding for zipcode\n",
    "# onehot encoding for initial call type\n",
    "# initial severity level code - use ordinal encoding instead of real\n",
    "# offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing\n",
    "### Dates: Year, Month, Week, Day, Hour\n",
    "### ZIPCODE: Onehot encoding\n",
    "### InitialCallType: Onehot encoding\n",
    "### Initial severity level code: Ordinal encoding\n",
    "### Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(\"../EMS_2020_1.csv\")\n",
    "feature_names = names(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x==NaN for x in df[:ZIPCODE]]) #the nans are counted in the onehot encoding..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out invalid samples. _ missing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[:VALID_DISPATCH_RSPNS_TIME_INDC] .== \"Y\", :]\n",
    "df = df[df[:VALID_INCIDENT_RSPNS_TIME_INDC] .== \"Y\", :];\n",
    "\n",
    "df[:INCIDENT_RESPONSE_SECONDS_QY] = string_to_float.(df[:, :INCIDENT_RESPONSE_SECONDS_QY])\n",
    "df[:ZIPCODE] = string.(df[:ZIPCODE]) #string.(convert.(Int, df[:ZIPCODE]))\n",
    "\n",
    "validFloat(x) = x::Float64 >=0\n",
    "validInt(x) = x::Int64 >= 0\n",
    "\n",
    "df = df[(validFloat.(df[!, :INCIDENT_RESPONSE_SECONDS_QY])), :];\n",
    "df = df[(validInt.(df[!, :FINAL_SEVERITY_LEVEL_CODE])), :];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = process_dates(df[:INCIDENT_DATETIME])\n",
    "\n",
    "#categorical, but there are many so maybe not the best option. Get supplementary data to join on zipcode.\n",
    "zipcodes = unique(df[:ZIPCODE])\n",
    "zips = onehot(df[:ZIPCODE], zipcodes)\n",
    "\n",
    "#categorical\n",
    "call_types = unique(df[:INITIAL_CALL_TYPE])\n",
    "icalltypes = onehot(df[:INITIAL_CALL_TYPE], call_types)\n",
    "\n",
    "#Ordinals. Does it matter if lowest is higher severity or highest is highest severity?\n",
    "isevs = df[:INITIAL_SEVERITY_LEVEL_CODE];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = hcat(dates, zips, icalltypes, isevs, ones(size(df, 1)))\n",
    "X = DataFrame(YEAR = dates[:, 1], MONTH = dates[:, 2], HOUR= dates[:, 5], ZIPCODES = df[:ZIPCODE], CALLTYPES = df[:INITIAL_CALL_TYPE], SEVERITYLEVEL = df[:INITIAL_SEVERITY_LEVEL_CODE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = df[:, :FINAL_SEVERITY_LEVEL_CODE]\n",
    "target2 = df[:, :INCIDENT_RESPONSE_SECONDS_QY];\n",
    "#data = df[:, filter(col -> (col != \"FINAL_SEVERITY_LEVEL_CODE\" && col != \"INCIDENT_RESPONSE_SECONDS_QY\"), feature_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you will split the data to create training and test sets. \n",
    "train_proportion = 0.8\n",
    "n = size(df, 1)\n",
    "println(\"Size of dataset: \", string(n))\n",
    "\n",
    "# Put the first ntrain observations in the DataFrame df into the training set, and the rest into the test set\n",
    "ntrain = convert(Int, round(train_proportion*n))\n",
    "\n",
    "# the following variable records the features of examples in the training set\n",
    "train_x = X[1:ntrain,:]\n",
    "# the following variable records the features of examples in the test set\n",
    "test_x = X[ntrain+1:end,:]\n",
    "# the following variable records the labels of examples in the training set\n",
    "train_y1 = target1[1:ntrain]\n",
    "train_y2 = target2[1:ntrain]\n",
    "# the following variable records the labels of examples in the test set\n",
    "test_y1 = target1[ntrain+1:end];\n",
    "test_y2 = target2[ntrain+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating number and percentage of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing = 0\n",
    "\n",
    "for i in 1:size(feature_names)[1]\n",
    "    feat = feature_names[i]\n",
    "    if (string(eltype(df[!, i])) == \"String\")\n",
    "        num_missing += sum((df[feat].==\"nan\"))\n",
    "    else\n",
    "        num_missing += sum(isnan.(df[feat]))\n",
    "    end\n",
    "end\n",
    "\n",
    "println(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, num_feats = size(df)\n",
    "\n",
    "println((n, num_feats))\n",
    "\n",
    "println(n*num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(num_missing/(n*num_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Response Time: Regression\n",
    "### Final Severity Level: Ordinal/MultiClass\n",
    "### Final Calltype: MultiClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree (Julia)\",\n",
    "         \"Random Forest (Julia)\", \"AdaBoost (Julia)\", \"Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "         \"Quadratic Discriminant Analysis\"]\n",
    "\n",
    "model = ()\n",
    "fit!(model, X, y)\n",
    "\n",
    "#for classifiers\n",
    "accuracy = sum(predict(model, X) .== y) / length(y)\n",
    "\n",
    "#for regression\n",
    "#MSE? MAE\n",
    "\n",
    "score = score(model, Xtest, ytest)\n",
    "\n",
    "predict_proba(model, X)\n",
    "decision_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AutoMLPipeline seems to do all this for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"AutoMLPipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AutoMLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = SKPreprocessor(\"StandardScaler\")\n",
    "ohe = OneHotEncoder()\n",
    "kohe = SKPreprocessor(\"OneHotEncoder\")\n",
    "catf = CatFeatureSelector()\n",
    "numf = NumFeatureSelector()\n",
    "disc = CatNumDiscriminator(5) # unique instances <= 5 are categories\n",
    "pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) \n",
    "dfcmc = fit_transform!(pcmc,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of learners and processors\n",
    "sklearners()\n",
    "skpreprocessors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skrf_reg = SKLearner(\"RandomForestRegressor\")\n",
    "skgb_reg =  SKLearner(\"GradientBoostingRegressor\")\n",
    "nothing #hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = DataFrame() \n",
    "for learner in [skrf_reg, skgb_reg]\n",
    "  pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) |> learner\n",
    "  println(learner.name)\n",
    "  mean,sd,folds,err = crossvalidate(pcmc,X,Y,\"accuracy_score\",5)\n",
    "  global learners = vcat(learners,DataFrame(name=learner.name,mean=mean,sd=sd,kfold=folds))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show learners;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
